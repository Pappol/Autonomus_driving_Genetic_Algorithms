{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZYjEZoi+4AblLEg/yUiCZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install gym --quiet\n",
        "! pip install highway-env --quiet\n",
        "! pip install neat-python --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoHubJE-r_om",
        "outputId": "1071b67d-a65d-46db-dcb5-3467b2f53b41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vp8-gLRcresC"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import highway_env\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import neat\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQNetwork(nn.Module):\n",
        "    def __init__(self,lr,input_dims,fc1_dims,fc2_dims,n_actions):\n",
        "        super(DeepQNetwork,self).__init__()\n",
        "        self.input_dims=input_dims\n",
        "        self.fc1_dims=fc1_dims\n",
        "        self.fc2_dims=fc2_dims\n",
        "        self.n_actions=n_actions\n",
        "\n",
        "        self.fc1=nn.Linear(self.input_dims,self.fc1_dims)\n",
        "        self.fc2=nn.Linear(self.fc1_dims,self.fc2_dims)\n",
        "        self.fc3=nn.Linear(self.fc2_dims,self.n_actions)\n",
        "\n",
        "        self.optimizer=optim.Adam(self.parameters(),lr=lr)\n",
        "        self.loss=nn.MSELoss()\n",
        "        self.device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self,state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        actions=self.fc3(x) \n",
        "        return actions\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self,gamma,epsilon,lr,input_dims,batch_size,n_actions,max_mem_size=100000,eps_end=0.01,eps_dec=5e-4):\n",
        "        \"\"\"\n",
        "\n",
        "        :param gamma: weight of future rewards\n",
        "        :param epsilon: explore/exploit dilemma ratio\n",
        "        :param lr: learning rate\n",
        "        :param batch_size: batch size of memory\n",
        "        :param n_actions: number of actions\n",
        "        :param max_mem_size: memory of past episodes\n",
        "        :param eps_end: once epsilon reaches this, training end\n",
        "        :param eps_dec: what to decrement epsilon at each timestamp\n",
        "        \"\"\"\n",
        "\n",
        "        self.gamma=gamma\n",
        "        self.epsilon=epsilon\n",
        "        self.eps_min=eps_end\n",
        "        self.eps_dec=eps_dec\n",
        "        self.lr=lr\n",
        "        self.action_space=[i for i in range(n_actions)]#List of available actions\n",
        "        self.mem_size=max_mem_size\n",
        "        self.batch_size=batch_size\n",
        "        self.mem_ctr=0 #Counter for array of memory\n",
        "\n",
        "        self.Q_eval=DeepQNetwork(self.lr,n_actions=n_actions,input_dims=input_dims,fc1_dims=256,fc2_dims=256)\n",
        "\n",
        "        self.state_memory=np.zeros((self.mem_size,input_dims),dtype=np.float32)\n",
        "        self.new_state_memory=np.zeros((self.mem_size,input_dims),dtype=np.float32) #Memory for next state\n",
        "\n",
        "        self.action_memory = np.zeros(self.mem_size,dtype=np.int32) #Memory for actions, stores int for index of action taken\n",
        "        self.reward_memory = np.zeros(self.mem_size,dtype=np.float32) #Memory for rewards\n",
        "        self.terminal_memory = np.zeros(self.mem_size,dtype=bool) #Memory for terminal states\n",
        "\n",
        "    def store_transition(self,state,action,reward,state_,done):\n",
        "        \"\"\"\n",
        "        Function for storing in memory each info about a state-action\n",
        "        :param state:\n",
        "        :param action:\n",
        "        :param reward:\n",
        "        :param state_:\n",
        "        :param done:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        index=self.mem_ctr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "\n",
        "        self.mem_ctr+=1\n",
        "\n",
        "    def choose_action(self,observation):\n",
        "        if np.random.random()>self.epsilon:\n",
        "            state= torch.tensor(observation).to(self.Q_eval.device)\n",
        "            actions=self.Q_eval.forward(state)\n",
        "            action=torch.argmax(actions).item() #Take action with highest activation output\n",
        "        else:\n",
        "            action=np.random.choice(self.action_space)\n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"\n",
        "        Once batch size is filled with episodes, learn from the episodes you have in batch\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if self.mem_ctr < self.batch_size: #Skip computation until batchsize is full\n",
        "            return\n",
        "        self.Q_eval.optimizer.zero_grad()\n",
        "        max_mem=min(self.mem_ctr,self.mem_size)\n",
        "        batch = np.random.choice(max_mem,self.batch_size,replace=False)\n",
        "\n",
        "        batch_index=np.arange(self.batch_size,dtype=np.int32)\n",
        "\n",
        "        state_batch = torch.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
        "        new_state_batch = torch.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
        "        reward_batch = torch.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
        "        terminal_batch=torch.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
        "\n",
        "        action_batch=self.action_memory[batch]\n",
        "\n",
        "        q_eval = self.Q_eval.forward(state_batch)[batch_index,action_batch]\n",
        "        q_next = self.Q_eval.forward(new_state_batch)\n",
        "        q_next[terminal_batch]=0.0\n",
        "\n",
        "        q_target=reward_batch+self.gamma*torch.max(q_next,dim=1)[0]\n",
        "\n",
        "        loss=self.Q_eval.loss(q_target,q_eval).to(self.Q_eval.device)\n",
        "        loss.backward()\n",
        "        self.Q_eval.optimizer.step()\n",
        "\n",
        "        #Decrease epsilon\n",
        "        self.epsilon=self.epsilon-self.eps_dec if self.epsilon > self.eps_min else self.eps_min"
      ],
      "metadata": {
        "id": "pnSWPoQ1toTl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def renderEnvironmentExample(env):\n",
        "    obs, info = env.reset()\n",
        "    env.render()\n",
        "    env.close()\n",
        "\n",
        "\n",
        "def list_envs():\n",
        "    all_envs = gym.envs.registry\n",
        "    print(sorted(all_envs))\n",
        "\n",
        "def eval_genomes(genomes, config):\n",
        "    for genome_id, genome in genomes:\n",
        "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
        "        observation = env.reset(seed=10)[0].flatten()\n",
        "        done = False\n",
        "        fitness = 0.0\n",
        "        while not done:\n",
        "            action = net.activate(observation)\n",
        "            #argmax the function\n",
        "            action = np.argmax(action)\n",
        "            observation_, reward, done, _, info = env.step(action)\n",
        "            observation_ = observation.flatten()\n",
        "            fitness += reward\n",
        "            observation = observation_\n",
        "        genome.fitness = fitness\n",
        "\n",
        "def learnDQNetwork(env, config_path):\n",
        "    agent = Agent(gamma=0.99, epsilon=1.0, batch_size=128, n_actions=5, eps_end=0.01, input_dims=25, lr=5e-4)\n",
        "    scores, epshistory = [], []\n",
        "\n",
        "    config = neat.config.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
        "                                neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
        "                                config_path)\n",
        "    \n",
        "    p = neat.Population(config)\n",
        "    p.add_reporter(neat.StdOutReporter(True))\n",
        "    stats = neat.StatisticsReporter()\n",
        "    p.add_reporter(stats)\n",
        "    winner = p.run(eval_genomes, 50)\n",
        "    print('\\nBest genome:\\n{!s}'.format(winner))"
      ],
      "metadata": {
        "id": "mNSuaIByrjtt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())\n",
        "#list_envs()\n",
        "env=gym.make(\"highway-fast-v0\", render_mode='rgb_array')\n",
        "config_path = 'config-ff.txt'\n",
        "\n",
        "learnDQNetwork(env, config_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNk97936rsbo",
        "outputId": "43c12198-de0d-4be3-8366-4adebd9cc50c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "\n",
            " ****** Running generation 0 ****** \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
            "  logger.warn(\n",
            "Warning: initial_connection = full with hidden nodes will not do direct input-output connections;\n",
            "\tif this is desired, set initial_connection = full_nodirect;\n",
            "\tif not, set initial_connection = full_direct\n",
            "Warning: initial_connection = full with hidden nodes will not do direct input-output connections;\n",
            "\tif this is desired, set initial_connection = full_nodirect;\n",
            "\tif not, set initial_connection = full_direct\n",
            "Warning: initial_connection = full with hidden nodes will not do direct input-output connections;\n",
            "\tif this is desired, set initial_connection = full_nodirect;\n",
            "\tif not, set initial_connection = full_direct\n",
            "Warning: initial_connection = full with hidden nodes will not do direct input-output connections;\n",
            "\tif this is desired, set initial_connection = full_nodirect;\n",
            "\tif not, set initial_connection = full_direct\n",
            "Warning: initial_connection = full with hidden nodes will not do direct input-output connections;\n",
            "\tif this is desired, set initial_connection = full_nodirect;\n",
            "\tif not, set initial_connection = full_direct\n",
            "Warning: initial_connection = full with hidden nodes will not do direct input-output connections;\n",
            "\tif this is desired, set initial_connection = full_nodirect;\n",
            "\tif not, set initial_connection = full_direct\n",
            "Warning: initial_connection = full with hidden nodes will not do direct input-output connections;\n",
            "\tif this is desired, set initial_connection = full_nodirect;\n",
            "\tif not, set initial_connection = full_direct\n",
            "Warning: initial_connection = full with hidden nodes will not do direct input-output connections;\n",
            "\tif this is desired, set initial_connection = full_nodirect;\n",
            "\tif not, set initial_connection = full_direct\n",
            "Warning: initial_connection = full with hidden nodes will not do direct input-output connections;\n",
            "\tif this is desired, set initial_connection = full_nodirect;\n",
            "\tif not, set initial_connection = full_direct\n",
            "Warning: initial_connection = full with hidden nodes will not do direct input-output connections;\n",
            "\tif this is desired, set initial_connection = full_nodirect;\n",
            "\tif not, set initial_connection = full_direct\n"
          ]
        }
      ]
    }
  ]
}